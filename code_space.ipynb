{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00cf6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fafc88cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.datasets import load_boston\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "147d2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_preprocessing:\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        from warnings import filterwarnings\n",
    "        filterwarnings(\"ignore\")\n",
    "        self.objects=data_preprocessing.initialize()\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        self.features = list(data.columns)\n",
    "        self.output_name = None\n",
    "        self.train_features,self.train_target,self.test_target,self.test_features = None,None,None,None\n",
    "    def drop_null(self):\n",
    "        self.data.dropna(axis=0,inplace=True)\n",
    "    def initialize():\n",
    "        from sklearn import preprocessing,model_selection,decomposition\n",
    "        return {\n",
    "                'Standard scaler':preprocessing.StandardScaler,\n",
    "                'Min Max Scalar':preprocessing.MinMaxScaler,\n",
    "                'PCA':decomposition.PCA,\n",
    "                'train test split':model_selection.train_test_split,\n",
    "               }\n",
    "    def out_in(self,output_name):\n",
    "        self.input = self.data.drop(output_name,axis=1)\n",
    "        self.output = self.data[output_name]\n",
    "        self.features.remove(output_name)\n",
    "        self.output_name = output_name\n",
    "    def apply_count_vectorize(self,col,count_vect_obj=None):\n",
    "        if count_vect_obj ==None:\n",
    "            from sklearn.feature_extraction.text import CountVectorizer\n",
    "            self.objects['Countvec_'+col] = CountVectorizer()\n",
    "            self.data[col] = self.objects['Countvec_'+col].fit_transform(data[col])\n",
    "        else:\n",
    "            self.objects['Countvec_'+col] = count_vect_obj\n",
    "            self.data[col] = self.objects['Countvec_'+col].fit_transform(data[col])\n",
    "    def split(self,test_percent,rs = 42):\n",
    "         self.train_features,self.test_features,self.train_target,self.test_target = self.objects['train test split'](self.input,self.output,test_size=test_percent,random_state=rs)\n",
    "    def get_object_column(self):\n",
    "        import numpy as np\n",
    "        edit_col = [i for i in self.features if self.data[i].dtype == np.object]\n",
    "        return edit_col\n",
    "    def encode_categorical_columns(self):\n",
    "        import numpy as np\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        label_encoder_objects ={}\n",
    "        edit_columns = self.get_object_column()\n",
    "        for col in edit_columns:\n",
    "            label_object = LabelEncoder()\n",
    "            self.data[col]=label_object.fit_transform(self.data[col])\n",
    "            label_encoder_objects[col+\"_encoder_object\"] = label_object\n",
    "        self.objects['Label_Encoder'] = label_encoder_objects\n",
    "    def change_columns(self,columns):\n",
    "        self.data = self.data[columns]\n",
    "    def apply_smote_data(self):\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        smote_object = SMOTE()\n",
    "        self.train_features,self.train_target = smote_object.fit_resample(self.train_features,self.train_target)\n",
    "        self.objects['Smote object'] = smote_object\n",
    "    def standardize_or_normalize(self,scale_type=None):\n",
    "        if scale_type == \"Standard\":\n",
    "            from pandas import DataFrame as df\n",
    "            scale_object  = self.objects['Standard scaler']()\n",
    "            self.train_features=df(data = scale_object.fit_transform(self.train_features),columns = self.features)\n",
    "            self.test_features = df(data = scale_object.fit_transform(self.test_features),columns = self.features)\n",
    "        elif scale_type == \"Normalize\":\n",
    "            from pandas import DataFrame as df\n",
    "            scale_object  = self.objects['Min Max Scalar']()\n",
    "            self.train_features=df(data = scale_object.fit_transform(self.train_features),columns = self.features)\n",
    "            self.test_features = df(data = scale_object.fit_transform(self.test_features),columns = self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "639161b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class machine_learning_classification:\n",
    "    def __init__(self,data_pr,models=None):\n",
    "        self.data = data_pr.data\n",
    "        self.train_features = data_pr.train_features\n",
    "        self.train_target = data_pr.train_target\n",
    "        self.test_features = data_pr.test_features\n",
    "        self.test_target = data_pr.test_target\n",
    "        if models==None:\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            from sklearn.tree import DecisionTreeClassifier\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            from sklearn.naive_bayes import BernoulliNB,GaussianNB\n",
    "            from sklearn.neighbors import KNeighborsClassifier\n",
    "            models = [LogisticRegression(),DecisionTreeClassifier(),RandomForestClassifier(),BernoulliNB(),GaussianNB(),KNeighborsClassifier()]\n",
    "        self.model_evaluvation_dict = {str(i).replace(\"()\",\"\"):{'model_object':i} for i in models}\n",
    "        self.model_prediction = {str(i).replace(\"()\",\"\"):None for i in models}\n",
    "    def fit(self):\n",
    "        for model,dic in self.model_evaluvation_dict.items():\n",
    "            self.model_evaluvation_dict[model]['model_object'].fit(self.train_features,self.train_target)\n",
    "            self.model_prediction[model] = self.model_evaluvation_dict[model]['model_object'].predict(self.test_features)\n",
    "    def Score_test_data(self):\n",
    "        for model,dic in self.model_evaluvation_dict.items():\n",
    "            self.model_evaluvation_dict[model]['score on test data'] = self.model_evaluvation_dict[model]['model_object'].score(self.test_features,self.test_target)*100\n",
    "    def create_confusion_matrix(self):\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        for model,dic in self.model_evaluvation_dict.items():\n",
    "            self.model_evaluvation_dict[model]['confusion matrix for test data'] = confusion_matrix(self.test_target,self.model_prediction[model]).tolist()\n",
    "    def create_f1_precision_recall(self):\n",
    "        from sklearn.metrics import f1_score,recall_score,precision_score\n",
    "        for model,dic in self.model_evaluvation_dict.items():\n",
    "            self.model_evaluvation_dict[model]['f1 score for test data'] = f1_score(self.test_target,self.model_prediction[model],average='macro')*100\n",
    "            self.model_evaluvation_dict[model]['precision for test data'] = precision_score(self.test_target,self.model_prediction[model],average='macro')*100\n",
    "            self.model_evaluvation_dict[model]['recall for test data'] = recall_score(self.test_target,self.model_prediction[model],average='macro')*100\n",
    "    def evaluvate(self):\n",
    "        self.fit()\n",
    "        self.Score_test_data()\n",
    "        self.create_confusion_matrix()\n",
    "        self.create_f1_precision_recall()\n",
    "        return self.model_evaluvation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "02ac6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class machine_learning_regression:\n",
    "    def __init__(self,data_pr,models=None):\n",
    "        self.data = data_pr.data\n",
    "        self.train_features = data_pr.train_features\n",
    "        self.train_target = data_pr.train_target\n",
    "        self.test_features = data_pr.test_features\n",
    "        self.test_target = data_pr.test_target\n",
    "        if models == None:\n",
    "            from sklearn.linear_model import LinearRegression,Ridge,Lasso\n",
    "            from sklearn.tree import DecisionTreeRegressor\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            from sklearn.neighbors import KNeighborsRegressor\n",
    "            models = [LinearRegression(),Ridge(),Lasso(),DecisionTreeRegressor(),RandomForestRegressor(),KNeighborsRegressor()]\n",
    "        self.model_evaluvation_dict = {str(i).replace(\"()\",\"\"):{'model_object':i} for i in models}\n",
    "        self.model_prediction = {str(i).replace(\"()\",\"\"):None for i in models}\n",
    "    def fit(self):\n",
    "        for model,dic in self.model_evaluvation_dict.items():\n",
    "            self.model_evaluvation_dict[model]['model_object'].fit(self.train_features,self.train_target)\n",
    "            self.model_prediction[model] = self.model_evaluvation_dict[model]['model_object'].predict(self.test_features)\n",
    "    def Score_test_dataset(self):\n",
    "        from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error,mean_absolute_percentage_error\n",
    "        metrics = {'r2 score':r2_score,'MAE':mean_absolute_error,'MSE':mean_squared_error,'MAPE':mean_absolute_percentage_error}\n",
    "        for model,dic in self.model_evaluvation_dict.items():\n",
    "            for metric,obj in metrics.items():\n",
    "                self.model_evaluvation_dict[model][metric] = obj(self.model_prediction[model],self.test_target)\n",
    "    def evaluvate(self):\n",
    "        self.fit()\n",
    "        self.Score_test_dataset()\n",
    "        return self.model_evaluvation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "077468ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = pd.DataFrame(data= load_breast_cancer().data,columns=load_breast_cancer().feature_names)\n",
    "data['target'] = load_breast_cancer().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ff7e34b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "deec2ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_object = data_preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "98be1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_object.drop_null()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e2402394",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_object.encode_categorical_columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "28875b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_object.out_in('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a810466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_object.split(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c6dd65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_object.apply_smote_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b5836e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_object.standardize_or_normalize(\"Standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "669dec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "420037d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj = machine_learning_classification(data_pre_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "174915ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': {'model_object': LogisticRegression(),\n",
       "  'score on test data': 94.73684210526315,\n",
       "  'confusion matrix for test data': [[62, 1], [8, 100]],\n",
       "  'f1 score for test data': 94.46343130553656,\n",
       "  'precision for test data': 93.79066478076379,\n",
       "  'recall for test data': 95.5026455026455},\n",
       " 'DecisionTreeClassifier': {'model_object': DecisionTreeClassifier(),\n",
       "  'score on test data': 86.54970760233918,\n",
       "  'confusion matrix for test data': [[62, 1], [22, 86]],\n",
       "  'f1 score for test data': 86.27943485086342,\n",
       "  'precision for test data': 86.33004926108374,\n",
       "  'recall for test data': 89.02116402116403},\n",
       " 'RandomForestClassifier': {'model_object': RandomForestClassifier(),\n",
       "  'score on test data': 94.15204678362574,\n",
       "  'confusion matrix for test data': [[63, 0], [10, 98]],\n",
       "  'f1 score for test data': 93.8963449457453,\n",
       "  'precision for test data': 93.15068493150685,\n",
       "  'recall for test data': 95.37037037037037},\n",
       " 'BernoulliNB': {'model_object': BernoulliNB(),\n",
       "  'score on test data': 94.73684210526315,\n",
       "  'confusion matrix for test data': [[62, 1], [8, 100]],\n",
       "  'f1 score for test data': 94.46343130553656,\n",
       "  'precision for test data': 93.79066478076379,\n",
       "  'recall for test data': 95.5026455026455},\n",
       " 'GaussianNB': {'model_object': GaussianNB(),\n",
       "  'score on test data': 94.73684210526315,\n",
       "  'confusion matrix for test data': [[62, 1], [8, 100]],\n",
       "  'f1 score for test data': 94.46343130553656,\n",
       "  'precision for test data': 93.79066478076379,\n",
       "  'recall for test data': 95.5026455026455},\n",
       " 'KNeighborsClassifier': {'model_object': KNeighborsClassifier(),\n",
       "  'score on test data': 96.49122807017544,\n",
       "  'confusion matrix for test data': [[63, 0], [6, 102]],\n",
       "  'f1 score for test data': 96.29870129870129,\n",
       "  'precision for test data': 95.65217391304348,\n",
       "  'recall for test data': 97.22222222222221}}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_obj.evaluvate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "96130d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "28267e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data = load_boston().data,columns = load_boston().feature_names)\n",
    "data['price'] = load_boston().target\n",
    "data.head(5)\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7f96f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_object = data_preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "86014706",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_object.drop_null()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bc0d6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_object.out_in('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f1bb0d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_object.split(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b1884b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_object.standardize_or_normalize('Normalize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "38376c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression,Lasso,Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "201ccb44",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = machine_learning_regression(data_pre_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1b8f071a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LinearRegression': {'model_object': LinearRegression(),\n",
       "  'r2 score': 0.6118020073614188,\n",
       "  'MAE': 4.258291450657327,\n",
       "  'MSE': 31.55304926006149,\n",
       "  'MAPE': 0.44593798219690534},\n",
       " 'Ridge': {'model_object': Ridge(),\n",
       "  'r2 score': 0.5862312682247668,\n",
       "  'MAE': 4.13586085968485,\n",
       "  'MSE': 29.230829688715502,\n",
       "  'MAPE': 0.7564603791846841},\n",
       " 'Lasso': {'model_object': Lasso(),\n",
       "  'r2 score': -13.462740665215257,\n",
       "  'MAE': 5.120079772025155,\n",
       "  'MSE': 53.7785447350872,\n",
       "  'MAPE': 0.22893222219685208},\n",
       " 'DecisionTreeRegressor': {'model_object': DecisionTreeRegressor(),\n",
       "  'r2 score': 0.7240057442204971,\n",
       "  'MAE': 3.8372549019607844,\n",
       "  'MSE': 24.255294117647058,\n",
       "  'MAPE': 0.17343816097097448},\n",
       " 'RandomForestRegressor': {'model_object': RandomForestRegressor(),\n",
       "  'r2 score': 0.6322439957591062,\n",
       "  'MAE': 4.423843137254901,\n",
       "  'MSE': 30.71795594117648,\n",
       "  'MAPE': 0.18108966001368573},\n",
       " 'KNeighborsRegressor': {'model_object': KNeighborsRegressor(),\n",
       "  'r2 score': 0.5523848171234557,\n",
       "  'MAE': 3.0907843137254902,\n",
       "  'MSE': 23.523619607843138,\n",
       "  'MAPE': 0.15312783270054825}}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.evaluvate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c804071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression Ridge Lasso RandomForestRegressor\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98e28bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e154b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]([5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab5333b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
